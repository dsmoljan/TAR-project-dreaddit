{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYN5VUXQQ8ix",
    "outputId": "1cd8dfcb-7d10-40c8-bc25-86ee0f8ef3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting empath\n",
      "  Downloading empath-0.89.tar.gz (57 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages (from empath) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->empath) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->empath) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->empath) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->empath) (3.3)\n",
      "Building wheels for collected packages: empath\n",
      "  Building wheel for empath (setup.py): started\n",
      "  Building wheel for empath (setup.py): finished with status 'done'\n",
      "  Created wheel for empath: filename=empath-0.89-py3-none-any.whl size=57824 sha256=661d561750728a829354f42116f35e301b57e40abe8e1df2637d491620d8de96\n",
      "  Stored in directory: c:\\users\\dsmoljan\\appdata\\local\\pip\\cache\\wheels\\5b\\58\\77\\7eed8eef4c6be0cca8920ac319d916811537a37407da220bf1\n",
      "Successfully built empath\n",
      "Installing collected packages: empath\n",
      "Successfully installed empath-0.89\n"
     ]
    }
   ],
   "source": [
    "!pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJH-GOPQOnjY",
    "outputId": "510648df-f701-435f-cae1-954ed2f0d878"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmoljan\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "root = \"/content/drive/MyDrive/TAR/\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "import pdb\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "SAVE_PATH = \"models\"\n",
    "directory_path = \"test-mlm\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "class DreadditData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        #self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "        self.data_id = dataframe.id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        data_id = self.data_id[index]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'data_id': torch.tensor(data_id, dtype=torch.int)\n",
    "        }\n",
    "\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self, add_train = True):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        if (add_train is True):\n",
    "            print(\"Initializing additionaly trained roberta model\")\n",
    "            self.roberta_layer = RobertaModel.from_pretrained(directory_path)\n",
    "        else:\n",
    "            print(\"Initializing basic roberta model\")\n",
    "            self.roberta_layer = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    # TODO: mozda train metodu stavi ovdje?\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        roberta_out = self.roberta_layer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = roberta_out[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        s = self.pre_classifier(pooler)\n",
    "        h = torch.nn.ReLU()(s)\n",
    "        h_dropout = self.dropout(h)\n",
    "        logits = self.classifier(h_dropout)\n",
    "        return logits\n",
    "    \n",
    "# metoda na osnovu prediction dataloadera vraća rječnik u kojem\n",
    "# je predikcija zadanog modela sa id-om posta\n",
    "def predict(model, prediction_loader):\n",
    "    prediction_dict = dict()\n",
    "    #test_acc_list = []\n",
    "    for i, data in tqdm(enumerate(prediction_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        #targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        post_ids = data['data_id'].cpu().numpy()\n",
    "        outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        big_idx = torch.max(outputs.data)\n",
    "        # y_true = targets.detach().cpu().numpy()\n",
    "        if (big_idx > 0.5):\n",
    "          y_pred = 1\n",
    "        else:\n",
    "          y_pred = 0\n",
    "        # print(\"\\ny_true\", y_true)\n",
    "        # print(\"y_pred\", y_pred)\n",
    "\n",
    "        post_id = post_ids[0]\n",
    "        prediction_dict.update({post_id:y_pred})\n",
    "    \n",
    "    #print(\"Accuracy on prediction: \", np.nanmean(test_acc_list))    \n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "36YLKcseRHfy"
   },
   "outputs": [],
   "source": [
    "def roberta_predict(df):\n",
    "  test_params = {'batch_size': 10,\n",
    "              'shuffle': True,\n",
    "                  'num_workers': 0\n",
    "              }\n",
    "      \n",
    "  model = RobertaClass(True)\n",
    "  model.load_state_dict(torch.load(\"roberta_cl_model.chkpt\")) \n",
    "  model.to(device)\n",
    "  tokenizer = RobertaTokenizer.from_pretrained(directory_path, padding = True, truncation=True, do_lower_case=True)\n",
    "  text = \"stres\"\n",
    "  id = 1\n",
    "  pred_data = df\n",
    "  #pred_data = pd.read_csv('/content/drive/MyDrive/TAR/to_predict.csv')\n",
    "  #pred_data = pred_data[['text', 'id']]\n",
    "\n",
    "  #print(pred_data.head)\n",
    "          \n",
    "  pred_set = DreadditData(pred_data, tokenizer, MAX_LEN)\n",
    "\n",
    "  prediction_loader = DataLoader(pred_set, **test_params)\n",
    "\n",
    "  predictions = predict(model, prediction_loader)\n",
    "  return (list(predictions.values())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9M_sDBLRbnD",
    "outputId": "860bdfc8-cee4-4ea6-c29b-44bd673782df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xMaEHbLIXxDT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from empath import Empath\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed(1)\n",
    "lexicon = Empath()\n",
    "\n",
    "def prepare_data(dataset_path_train = None,):\n",
    "    if (dataset_path_train is None):\n",
    "        raise RuntimeException(\"Error! Dataset must be provided\")\n",
    "    train = pd.read_csv(dataset_path_train)\n",
    "    y_train = train['label']\n",
    "    X_train = train[['roberta_prediction', 'text']]\n",
    "    return X_train, y_train\n",
    "\n",
    "def empathfeats(X, train=False, sclr=None):\n",
    "    rows = []\n",
    "    for t in X['text']:\n",
    "        empath = lexicon.analyze(t, normalize=True)\n",
    "        rows.append(pd.DataFrame([empath]))\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    X = pd.concat([X,df], axis=1)\n",
    "    \n",
    "    X = X.drop(columns=['text'])\n",
    "    to_drop = ['health','banking','night','college','exasperation','reading','worship','eating','water','legend','neglect','swimming','love','sympathy','vehicle','disgust','sound','sailing','warmth','fun','joy','affection','lust','shame','anger','car','technology','power','white_collar_job','party','cleaning','competing','ocean','contentment','musical']\n",
    "    X = X.drop(columns=to_drop)\n",
    "    \n",
    "    if(train): \n",
    "        sclr = StandardScaler()\n",
    "        X = sclr.fit_transform(X)\n",
    "    else:\n",
    "        X = sclr.transform(X)\n",
    "    return X, sclr\n",
    "\n",
    "def robertafeat(text):\n",
    "    df = pd.DataFrame({'id': [0],'text': [text]})\n",
    "    prediction = roberta_predict(df)\n",
    "    return prediction\n",
    "    \n",
    "def train_model():\n",
    "    X_train, y_train = prepare_data(dataset_path_train = 'train_pred_mental_health.csv')\n",
    "    X_train, sclr = empathfeats(X_train, train=True)\n",
    "    hp = {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "    model = LogisticRegression(max_iter=1000000, **hp)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, sclr\n",
    "\n",
    "def make_prediction(text, model, sclr):\n",
    "    trans, sclr = empathfeats(pd.DataFrame({'roberta_prediction': [robertafeat(text)], 'text': [text]}), sclr=sclr)\n",
    "    return model.predict(trans)\n",
    "\n",
    "def final_prediction(text):\n",
    "    global model, sclr\n",
    "    return make_prediction(text, model, sclr)[0]\n",
    "\n",
    "model, sclr = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xMaEHbLIXxDT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zilo_CgZX0ea"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I hate my life, i want to die\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing additionaly trained roberta model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test-mlm were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at test-mlm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressful\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I love my life\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing additionaly trained roberta model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test-mlm were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at test-mlm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not stressful\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I've been staying off social media and have avoided everything related to news about the Texas massacre, so that I don't go crazy. Well outside of knowing the basic details anyway (that there was a shooting a school in Texas and children were killed). It may sound selfish to avoid the news, but I can't handle it...I'm completely at my limit with all the back to back traumatic things happening across the world.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing additionaly trained roberta model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at test-mlm were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at test-mlm and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stressful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "while(True):\n",
    "    txt = input()\n",
    "    pred = final_prediction(txt)\n",
    "    if pred == 0:\n",
    "        print(\"Not stressful\")\n",
    "    else:\n",
    "        print(\"Stressful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TAR demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
